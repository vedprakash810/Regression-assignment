{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax8UTAFrLgIJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "\n",
        "Ans-Simple linear regression is used to estimate the relationship between two quantitative variables.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Ans- The key assumptions of Simple Linear Regression are linearity, independence of errors, homoscedasticity, and normality of residuals. These assumptions are crucial for the validity of the regression model and its results.\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "Ans- In the equation Y = mX + c, the coefficient \"m\" represents the slope or gradient of the line. The slope indicates how steep the line is and can be positive, negative, or zero.\n",
        "\n",
        "4.  What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "Ans- In the equation Y = mX + c, the intercept \"c\" represents the y-intercept, which is the point where the line crosses the y-axis. Specifically, \"c\" is the value of Y when X = 0.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "Ans- In simple linear regression, the slope m represents the change in the dependent variable (Y) for every one unit change in the independent variable (X). It's calculated by dividing the covariance of X and Y by the variance of X, often expressed as m = r * (sy/sx), where r is the correlation coefficient, sy is the standard deviation of Y, and sx is the standard deviation of X.\n",
        "\n",
        "6.  What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "Ans- The purpose of the least squares method in simple linear regression is to find the line of best fit that minimizes the sum of the squared differences between the observed data points and the corresponding points on the regression line.\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "Ans- In Simple Linear Regression, the coefficient of determination (R²) indicates the proportion of the variance in the dependent variable that is explained by the independent variable.\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "\n",
        "Ans- Multiple linear regression is a statistical technique that predicts a single dependent variable by using two or more independent variables, modeling the relationship between them as a linear combination.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Ans- The primary difference between simple and multiple linear regression lies in the number of independent variables used to predict a dependent variable: simple regression uses one, while multiple regression uses two or more.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Ans- The key assumptions of Multiple Linear Regression include: linearity, independence of observations, homoscedasticity, multivariate normality, and no multicollinearity.\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Ans- Heteroscedasticity in a Multiple Linear Regression model refers to a situation where the variance of the error terms is not constant across all levels of the independent variables. This violates a key assumption of linear regression, leading to unreliable results, particularly in hypothesis testing and confidence interval estimation.\n",
        "\n",
        "12.  How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Ans- To improve a Multiple Linear Regression model with high multicollinearity, you can remove one of the correlated variables, combine them into a single variable, use dimensionality reduction techniques like Principal Component Analysis (PCA), or employ regularization methods such as Ridge or Lasso regression. Additionally, increasing the sample size can help reduce the impact of multicollinearity.\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Ans- Several techniques can transform categorical variables for use in regression models, including one-hot encoding, label encoding, and binary encoding. These methods convert categorical data into numerical representations that regression models can use for prediction.\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Ans- In multiple linear regression, interaction terms examine if the relationship between a predictor and the outcome changes depending on the values of another predictor, indicating a non-additive effect.\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "Ans- The intercept in simple linear regression represents the expected value of the dependent variable when the independent variable is zero. In multiple linear regression, the intercept represents the expected value of the dependent variable when all independent variables are zero. The interpretation of the intercept difference boils down to whether the zero values of the independent variables are meaningful or not, and whether you're interested in the relationship between predictors and the response, or simply making predictions.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "Ans- In regression analysis, the slope represents the change in the dependent variable for every one-unit change in the independent variable. A positive slope indicates a positive relationship, meaning as the independent variable increases, the dependent variable also increases. A negative slope indicates a negative relationship, where an increase in the independent variable leads to a decrease in the dependent variable. The magnitude of the slope determines the strength of this relationship; a steeper slope signifies a stronger relationship.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "Ans- In a regression model, the intercept represents the predicted value of the dependent variable when all independent variables are zero. It provides a baseline or starting point for understanding the relationship between variables, especially when the independent variable(s) can logically take on a value of zero.\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "Ans- R² (coefficient of determination) can be a misleading sole measure of model performance because it only reflects the proportion of variance explained by the model, not necessarily its overall goodness of fit or predictive accuracy. While a high R² suggests the model explains a significant amount of variability, it doesn't guarantee the model is reliable, unbiased, or suitable for prediction.\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "Ans- A large standard error for a regression coefficient suggests that the estimated coefficient is likely to be variable and less precise. It indicates that the estimate of the coefficient might differ significantly if the regression model was estimated using a different sample of data. This means there's less confidence in the reliability of the estimated coefficient as a predictor.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Ans- Heteroscedasticity, the unequal spread of residuals, is identified in residual plots by observing a \"fan\" or \"cone\" shape, where the spread of residuals changes systematically as the fitted values increase or decrease.\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "Ans- A high R² (coefficient of determination) but low adjusted R² in a multiple linear regression model suggests that the model fits the data well on the sample, but the added predictors don't contribute significantly to the model's overall explanatory power when considering the complexity of the model.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Ans- Scaling variables in Multiple Linear Regression is important to ensure all variables contribute equally to the model and for easier interpretation of coefficients. Scaling helps prevent larger magnitude variables from dominating the model, improves numerical stability, and allows for more meaningful comparisons of variable effects.\n",
        "\n",
        "23. What is polynomial regression?\n",
        "\n",
        "Ans- Polynomial regression is an extension of linear regression that uses higher-degree polynomial functions of the independent variable to model non-linear relationships between variables, allowing for more complex curves to be fit to data.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "Ans- Polynomial regression extends linear regression by allowing for curved relationships between variables, achieved by adding polynomial terms (like squared or cubed versions of the independent variable) to the model, whereas linear regression models a straight-line relationship.\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "\n",
        "Ans- Polynomial regression is used when the relationship between variables in a data set is non-linear, meaning a straight line won't fit the data well.\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "\n",
        "Ans- The general equation for polynomial regression is: y = b₀ + b₁x + b₂x² + ... + bₙxⁿ + ε where 'y' is the dependent variable, 'x' is the independent variable, 'b₀' to 'bₙ' are the coefficients to be estimated, 'n' is the degree of the polynomial, and 'ε' is the error term.\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Ans-Yes, polynomial regression can be applied to multiple variables. It's an extension of multiple linear regression that allows for non-linear relationships between the independent variables and the dependent variable.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "\n",
        "Ans- Polynomial regression, while useful for modeling non-linear relationships, has limitations like the risk of overfitting, especially with high-degree polynomials. It can also lead to increased computational complexity and challenges in selecting the optimal polynomial degree. Furthermore, it may not extrapolate well beyond the observed data range, and high-degree polynomials can be harder to interpret.\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "Ans- Several methods can be used to evaluate model fit when selecting the degree of a polynomial, including visual inspection, cross-validation, and analysis of variance (ANOVA). Visual inspection involves plotting the data and fitted curves with varying degrees to assess how well the model captures the underlying pattern without excessive complexity. Cross-validation splits the data into training and validation sets, allowing for model evaluation on unseen data and identification of overfitting. ANOVA compares the fit of models with different polynomial degrees to determine if adding higher-order terms significantly improves the model's explanatory power.\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        "\n",
        "Ans- Visualization is crucial in polynomial regression because it helps assess the model's fit to the data, identify potential overfitting, and understand the relationships between variables.\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        "\n",
        "Ans- The polynomial regression model is then trained by adjusting the coefficients of the polynomial terms to minimize the difference between the observed and predicted values of the dependent variable. The resulting equation can then be used to make predictions for new data."
      ],
      "metadata": {
        "id": "HnBbaTGTLq2G"
      }
    }
  ]
}